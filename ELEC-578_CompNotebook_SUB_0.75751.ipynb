{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a38a23b",
   "metadata": {},
   "source": [
    "# DD-SV Neuron Synapse Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66105ff6",
   "metadata": {},
   "source": [
    "## Load in and Wrangle Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2237b6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './train_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/DomDuliepre/Desktop/ELEC-578/ELEC-578_COMP./neuron-synapse-prediction/SUBs/ELEC-578_CompNotebook_SUB_0.75751.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/DomDuliepre/Desktop/ELEC-578/ELEC-578_COMP./neuron-synapse-prediction/SUBs/ELEC-578_CompNotebook_SUB_0.75751.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/DomDuliepre/Desktop/ELEC-578/ELEC-578_COMP./neuron-synapse-prediction/SUBs/ELEC-578_CompNotebook_SUB_0.75751.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#load in training data on each potential synapse\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/DomDuliepre/Desktop/ELEC-578/ELEC-578_COMP./neuron-synapse-prediction/SUBs/ELEC-578_CompNotebook_SUB_0.75751.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m./train_data.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/DomDuliepre/Desktop/ELEC-578/ELEC-578_COMP./neuron-synapse-prediction/SUBs/ELEC-578_CompNotebook_SUB_0.75751.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#load in additional features for each neuron\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/DomDuliepre/Desktop/ELEC-578/ELEC-578_COMP./neuron-synapse-prediction/SUBs/ELEC-578_CompNotebook_SUB_0.75751.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m feature_weights \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39m./feature_weights.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_engine(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmemory_map\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mencoding_errors\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstorage_options\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[39m=\u001b[39mioargs\u001b[39m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#load in training data on each potential synapse\n",
    "data = pd.read_csv(\"./train_data.csv\")\n",
    "\n",
    "#load in additional features for each neuron\n",
    "feature_weights = pd.read_csv(\"./feature_weights.csv\")\n",
    "morph_embeddings = pd.read_csv(\"./morph_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4070b48",
   "metadata": {},
   "source": [
    "#### Basic info on these dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21377c2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"Main Dataframe Size:\",data.shape)\n",
    "print(\"\\nMain Dataframe:\")\n",
    "data.info()\n",
    "\n",
    "print(\"\\nFeature Weights Size:\",feature_weights.shape)\n",
    "print(\"\\nFeature Weights:\")\n",
    "feature_weights.info(verbose=False)\n",
    "\n",
    "print(\"\\nMorphological Embeddings Size:\",morph_embeddings.shape)\n",
    "print(\"\\nMorphological Embeddings:\")\n",
    "morph_embeddings.info(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157b5f4",
   "metadata": {},
   "source": [
    "See the documentation for additional information on these features.  The main dataframe contains features for each potential synapse (each row).  The outcome to predict is 'connected'.  Note that many of the raw features in this dataframe will not be directly useful for prediction (e.g. x,y,z coordiantes, pre- and post-synapse neuron ID and etc.).  You will need to creatively engineer new features that will be more useful for prediction.\n",
    "\n",
    "The additional information on feature weights and morphological embeddings are given for each separate neuron (the neuron ID is matched with that in the main dataframe).  These will need to be merged with the other data to create training and test sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49737cbf",
   "metadata": {},
   "source": [
    "#### Merge Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83752d",
   "metadata": {},
   "source": [
    "Before merging, we concatenate the feature weights and morphological embeddings into np.array's to decrease memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a78bac9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# join all feature_weight_i columns into a single np.array column\n",
    "feature_weights[\"feature_weights\"] = (\n",
    "    feature_weights.filter(regex=\"feature_weight_\")\n",
    "    .sort_index(axis=1)\n",
    "    .apply(lambda x: np.array(x), axis=1)\n",
    ")\n",
    "# delete the feature_weight_i columns\n",
    "feature_weights.drop(\n",
    "    feature_weights.filter(regex=\"feature_weight_\").columns, axis=1, inplace=True\n",
    ")\n",
    "\n",
    "# join all morph_embed_i columns into a single np.array column\n",
    "morph_embeddings[\"morph_embeddings\"] = (\n",
    "    morph_embeddings.filter(regex=\"morph_emb_\")\n",
    "    .sort_index(axis=1)\n",
    "    .apply(lambda x: np.array(x), axis=1)\n",
    ")\n",
    "# delete the morph_embed_i columns\n",
    "morph_embeddings.drop(\n",
    "    morph_embeddings.filter(regex=\"morph_emb_\").columns, axis=1, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87211b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data = (\n",
    "    data.merge(\n",
    "        feature_weights.rename(columns=lambda x: \"pre_\" + x), \n",
    "        how=\"left\", \n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    "    .merge(\n",
    "        feature_weights.rename(columns=lambda x: \"post_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    "    .merge(\n",
    "        morph_embeddings.rename(columns=lambda x: \"pre_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    "    .merge(\n",
    "        morph_embeddings.rename(columns=lambda x: \"post_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210ce030",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"Data Size:\",data.shape)\n",
    "print(\"\\nData:\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798015c5",
   "metadata": {},
   "source": [
    "## Basic Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdb53dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc227ee",
   "metadata": {},
   "source": [
    "### Unbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660421ac",
   "metadata": {},
   "source": [
    "The outcomes, the adps that form a connection or synapse ('connected'==True) and the adps that do not synapse ('connected'==False), are unbalanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ff436c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"Number Connected (Synapses):\",data['connected'].sum())\n",
    "\n",
    "print(\"\\nNumber Not Connected:\",(~data['connected']).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abf1022",
   "metadata": {},
   "source": [
    "The number of presynaptic neurons and postsynaptic neurons are also unbalanced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8917f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(f\"All the adps are from {data['pre_nucleus_id'].nunique()} pre- neurons and {data['post_nucleus_id'].nunique()} post- neurons.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c02cc6",
   "metadata": {},
   "source": [
    "### Exploration: Synaptic Proximity Distance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6163e8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sns.catplot(data=data, x='connected', y='adp_dist', kind='boxen')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f50d61",
   "metadata": {},
   "source": [
    "Neurons that form a synpase ('connected') tend to be closer together (smaller distance between pre- and post-synaptic ADP - 'adp_dist')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6717c6",
   "metadata": {},
   "source": [
    "## Example: Basic Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d306806",
   "metadata": {},
   "source": [
    "### Exploration: Neurons that form a synpase tend to have similar functional neural responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc6105",
   "metadata": {},
   "source": [
    "Neurons in the visual cortex each respond differently to visual stimuli.  This is often captured by a neuron's tuning function, or the visual features that drive a neuron to fire.  The feature weight vectors ('feature_weights') are a kind of summary of each neuron's tuning function.  Neuroscience suggests that neurons with similar tuning are slightly more likely to be connected.  To assess this, we would need to engineer a new feature that capture the tuning similarity of the pre- and post- synaptic neurons for each ADP.  (See the documentation for more details).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6679b5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#cosine similarity function\n",
    "def row_feature_similarity(row):\n",
    "    pre = row[\"pre_feature_weights\"]\n",
    "    post = row[\"post_feature_weights\"]\n",
    "    return (pre * post).sum() / (np.linalg.norm(pre) * np.linalg.norm(post))\n",
    "\n",
    "def xCoorDifference(row):\n",
    "    dendriticX = row[\"dendritic_coor_x\"]\n",
    "    axonalX = row[\"axonal_coor_x\"]\n",
    "    return abs(dendriticX - axonalX)\n",
    "def yCoorDifference(row):\n",
    "    dendriticY = row[\"dendritic_coor_y\"]\n",
    "    axonalY = row[\"axonal_coor_y\"]\n",
    "    return abs(dendriticY - axonalY)\n",
    "def zCoorDifference(row):\n",
    "    dendriticZ = row[\"dendritic_coor_z\"]\n",
    "    axonalZ = row[\"axonal_coor_z\"]\n",
    "    return abs(dendriticZ - axonalZ)\n",
    "\n",
    "def EDax_denDist(row):\n",
    "#def xCoorDifference(row):\n",
    "    dendriticX = row[\"dendritic_coor_x\"]\n",
    "    axonalX = row[\"axonal_coor_x\"]\n",
    "    #return abs(dendriticX - axonalX)\n",
    "#def yCoorDifference(row):\n",
    "    dendriticY = row[\"dendritic_coor_y\"]\n",
    "    axonalY = row[\"axonal_coor_y\"]\n",
    "    #return abs(dendriticY - axonalY)\n",
    "#def zCoorDifference(row):\n",
    "    dendriticZ = row[\"dendritic_coor_z\"]\n",
    "    axonalZ = row[\"axonal_coor_z\"]\n",
    "    #return abs(dendriticZ - axonalZ)\n",
    "    return ((dendriticX - axonalX)**2 \n",
    "            + (dendriticY - axonalY)**2 \n",
    "            + (dendriticZ - axonalZ)**2)**(1/2)\n",
    "\n",
    "def EDpre_postnucleusDist(row):\n",
    "    preNucleusX = row[\"pre_nucleus_x\"]\n",
    "    preNucleusY = row[\"pre_nucleus_y\"]\n",
    "    preNucleusZ = row[\"pre_nucleus_z\"]\n",
    "\n",
    "    postNucleusX = row[\"post_nucleus_x\"]\n",
    "    postNucleusY = row[\"post_nucleus_y\"]\n",
    "    postNucleusZ = row[\"post_nucleus_z\"]\n",
    "    return np.sqrt((preNucleusX-postNucleusX)**2 \n",
    "                   + (preNucleusY-postNucleusY)**2 \n",
    "                   + (preNucleusZ-postNucleusZ)**2)\n",
    "\n",
    "def thetaDiff(row):\n",
    "    pre_orientation_vector_X = row[\"axonal_coor_x\"] - row[\"pre_nucleus_x\"]\n",
    "    pre_orientation_vector_Y = row[\"axonal_coor_y\"] - row[\"pre_nucleus_y\"]\n",
    "    pre_orientation_vector_Z = row[\"axonal_coor_z\"] - row[\"pre_nucleus_z\"]\n",
    "\n",
    "    post_orientation_vector_X = row[\"dendritic_coor_x\"] - row[\"post_nucleus_x\"]\n",
    "    post_orientation_vector_Y = row[\"dendritic_coor_y\"] - row[\"post_nucleus_y\"]\n",
    "    post_orientation_vector_Z = row[\"dendritic_coor_z\"] - row[\"post_nucleus_z\"]\n",
    "    \n",
    "    numerator = (pre_orientation_vector_X\n",
    "                 * post_orientation_vector_X\n",
    "                 + pre_orientation_vector_Y * post_orientation_vector_Y\n",
    "                 + pre_orientation_vector_Z * post_orientation_vector_Z\n",
    "                 )\n",
    "    denominator = (np.sqrt(pre_orientation_vector_X**2\n",
    "                    + pre_orientation_vector_Y**2\n",
    "                    + pre_orientation_vector_Z**2)\n",
    "                    * (np.sqrt(post_orientation_vector_X**2\n",
    "                    + post_orientation_vector_Y**2\n",
    "                    + post_orientation_vector_Z**2))\n",
    "                   )\n",
    "    cos_theta = numerator / denominator\n",
    "    actual_angle = np.arccos(cos_theta)*(180/np.pi)\n",
    "    return actual_angle\n",
    "\n",
    "#cosine similarity function\n",
    "def row_morphEmbed_similarity(row):\n",
    "    pre = row[\"pre_morph_embeddings\"]\n",
    "    post = row[\"post_morph_embeddings\"]\n",
    "    return (pre * post).sum() / (np.linalg.norm(pre) * np.linalg.norm(post))\n",
    "\n",
    "# location slicer relative to lop left of monitor\n",
    "def preRadialLocation(row):\n",
    "    pre = row[\"pre_rf_x\"]\n",
    "    post = row[\"pre_rf_y\"]\n",
    "    return np.sqrt(pre**2 * post**2)\n",
    "\n",
    "def postRadialLocation(row):\n",
    "    pre = row[\"post_rf_x\"]\n",
    "    post = row[\"post_rf_y\"]\n",
    "    return np.sqrt(pre**2 * post**2)\n",
    "\n",
    "def oracleDiff(row):\n",
    "    pre = row[\"pre_oracle\"]\n",
    "    post = row[\"post_oracle\"]\n",
    "    return pre * post\n",
    "\n",
    "def testScoreDiff(row):\n",
    "    pre = row[\"pre_test_score\"]\n",
    "    post = row[\"post_test_score\"]\n",
    "    return pre * post\n",
    "\n",
    "# compute the cosine similarity between the pre- and post- feature weights\n",
    "data[\"fw_similarity\"] = data.apply(row_feature_similarity, axis=1)\n",
    "# compute the cosine similarity between the pre- and post- morphological embeddings\n",
    "data[\"morphEm_similarity\"] = data.apply(row_morphEmbed_similarity, axis=1)\n",
    "# compute the difference between the X-coordinates\n",
    "data[\"coorX_difference\"] = data.apply(xCoorDifference, axis=1)\n",
    "# compute the difference between the Y-coordinates\n",
    "data[\"coorY_difference\"] = data.apply(yCoorDifference, axis=1)\n",
    "# compute the difference between the Z-coordinates\n",
    "data[\"coorZ_difference\"] = data.apply(zCoorDifference, axis=1)\n",
    "# Theta between 2 neurons\n",
    "data[\"theta_btwn_2neurons\"] = data.apply(thetaDiff, axis =1)\n",
    "# 3D distance between axon and dendrite \n",
    "data[\"3Dax_denDist\"] = data.apply(EDax_denDist, axis=1)\n",
    "# 3D distance between pre and post nucleus distance\n",
    "data[\"3Dpre_postNucleusDist\"] = data.apply(EDpre_postnucleusDist, axis=1)\n",
    "# Pre radial location\n",
    "data[\"preRadialLocation\"] = data.apply(preRadialLocation, axis=1)\n",
    "# Post radial location\n",
    "data[\"postRadialLocation\"] = data.apply(postRadialLocation, axis=1)\n",
    "# Combined Oracle Score of Pre-synaptic and Post-synaptic Neurons:\n",
    "data[\"Combined_oracle\"] = data.apply(oracleDiff, axis=1)\n",
    "# pre and post test score difference \n",
    "data[\"Combined_testScore\"] = data.apply(testScoreDiff, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd80efe",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#visualize these results\n",
    "sns.catplot(data=data, x='connected', y='fw_similarity', kind='boxen')\n",
    "sns.catplot(data=data, x='connected', y='coorX_difference', kind='boxen')\n",
    "sns.catplot(data=data, x='connected', y='coorY_difference', kind='boxen')\n",
    "sns.catplot(data=data, x='connected', y='coorZ_difference', kind='boxen')\n",
    "sns.catplot(data=data, x='connected', y='theta_btwn_2neurons', kind='boxen')\n",
    "sns.catplot(data=data, x='connected', y='3Dax_denDist', kind='boxen')\n",
    "sns.catplot(data=data, x='connected', y='3Dpre_postNucleusDist', kind='boxen')\n",
    "sns.catplot(data=data, x='connected', y='preRadialLocation', kind='boxen')\n",
    "sns.catplot(data=data, x='connected', y='postRadialLocation', kind='boxen')\n",
    "sns.catplot(data=data, x='connected', y='Combined_oracle', kind='boxen')\n",
    "sns.catplot(data=data, x='connected', y='Combined_testScore', kind='boxen')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01035751",
   "metadata": {},
   "source": [
    "There may be a very, very slight difference, but we can investigate this further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106744b9",
   "metadata": {},
   "source": [
    "### Exploration: Similar functional nerual responses by projection region in the brain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3de1b",
   "metadata": {},
   "source": [
    "Projection regions indicate the brain regions where the pre-synaptic and post-synaptic neurons are located in the brain.  Each brain region is responsible for different neural functions.  We can engineer a new feature to capture the projection regions and perhaps see if similar neural tuning is affiliated with connecitons in particular projection regions.  For example, it has been shown that connected neurons have slightly more similar neural tuning than unconnected neurons in the V1->V1 projection region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29908dfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# generate projection group as pre->post\n",
    "data[\"projection_group\"] = (\n",
    "    data[\"pre_brain_area\"].astype(str)\n",
    "    + \"->\"\n",
    "    + data[\"post_brain_area\"].astype(str)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b83b7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# check the distribution of the cosine similarity for projections groups with more than 100 synapses\n",
    "projection_group_counts = data.query('connected')['projection_group'].value_counts()\n",
    "projection_group_counts = projection_group_counts[projection_group_counts > 100].index\n",
    "sns.displot(\n",
    "    data=data.query('projection_group in @projection_group_counts'), \n",
    "    x='fw_similarity', \n",
    "    hue='connected', \n",
    "    row='projection_group',\n",
    "    common_norm=False, \n",
    "    stat='probability',\n",
    "    binwidth=.02,\n",
    "    height=2,\n",
    "    aspect=4,\n",
    "    facet_kws={'sharey': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1582585b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sns.catplot(data=data, x='projection_group', y='fw_similarity', hue='connected', kind='bar', alpha=.7)\n",
    "plt.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc827ab",
   "metadata": {},
   "source": [
    "We have shown some simple examples of how to engineer new features and also shown that these are related to neuron connectivity.  You will likley need to explore and engineer many other possible features in this competition.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a78d7f6",
   "metadata": {},
   "source": [
    "## Example: Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937655df",
   "metadata": {},
   "source": [
    "Now that we have shown some features are associated with neuron connectivity, we fit a simple calssification model: Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54850f42",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# logistic regression model (connected ~ fw_similarity + adp)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# split into a train and test set \n",
    "#(Even though we're working with the competition training set, you may want to have your own internal train and test sets.)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=1)\n",
    "\n",
    "# create pipeline\n",
    "pipe = Pipeline(\n",
    "    [(\"scaler\", StandardScaler()), (\"model\", LogisticRegression(random_state=2))]\n",
    ")\n",
    "\n",
    "# fit model\n",
    "pipe.fit(train_data[[\"fw_similarity\", \"adp_dist\"]], train_data[\"connected\"])\n",
    "\n",
    "# predict on test data\n",
    "test_data[\"pred\"] = pipe.predict_proba(test_data[[\"fw_similarity\", \"adp_dist\"]])[:, 1]\n",
    "\n",
    "# compute accuracy\n",
    "print(f\"accuracy: {accuracy_score(test_data['connected'], test_data['pred'] > .5)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2397d5",
   "metadata": {},
   "source": [
    "Wow.  This looks like terrific accuracy.  But, remember the data is unbalanced.  Let's check the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b351fdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "confusion_matrix(test_data['connected'], test_data['pred'] > .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4faf555",
   "metadata": {},
   "source": [
    "The model always predicts not connected!  We need a new evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5157b7fb",
   "metadata": {},
   "source": [
    "### Balanced Accuracy Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83390f1",
   "metadata": {},
   "source": [
    "Sensitivity = True Positive Rate = True Positive / (True Positive + False Negative)\n",
    "Specificity = True Negative Rate = True Negative / (True Negative + False Positive)\n",
    "\n",
    "Balanced Accuracy = (Sensitivity + Specificity)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2481e7b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"balanced accuracy: {balanced_accuracy_score(test_data['connected'], test_data['pred'] > .5)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5947b122",
   "metadata": {},
   "source": [
    "## Data Augmentation for Unbalanced Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b933c",
   "metadata": {},
   "source": [
    "As our simple model always predicts not connected, we need to use some technique that balances the two classes.  While there are many strategies one can employ, we demostrate a simple random over sampling strategy that uses the bootstrap to augment the data for the minority ('connected') class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c9e7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# oversample connected neuron pairs\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_resample(\n",
    "    train_data[[\"fw_similarity\", \"adp_dist\"]], train_data[\"connected\"]\n",
    ")\n",
    "\n",
    "# fit model\n",
    "pipe.fit(X_resampled, y_resampled)\n",
    "\n",
    "# predict on test data\n",
    "test_data[\"pred\"] = pipe.predict_proba(test_data[[\"fw_similarity\", \"adp_dist\"]])[:, 1]\n",
    "\n",
    "# compute accuracy\n",
    "print(f\"accuracy: {accuracy_score(test_data['connected'], test_data['pred'] > .5)}\")\n",
    "\n",
    "# confusion matrix\n",
    "print(confusion_matrix(test_data['connected'], test_data['pred'] > .5))\n",
    "\n",
    "# compute balanced accuracy\n",
    "print(\n",
    "    f\"balanced accuracy: {balanced_accuracy_score(test_data['connected'], test_data['pred'] > .5)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d1a156",
   "metadata": {},
   "source": [
    "Simple data augmentation improves our balanced accuracy dramatically!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb82ab85",
   "metadata": {},
   "source": [
    "# Create Example Prediction File for Leaderboard Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d12af71",
   "metadata": {},
   "source": [
    "### Load and Merge Leaderboard Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bdc8d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#we need to first load and merge the leaderboard data to have the same format as the training set\n",
    "lb_data = pd.read_csv(\"./leaderboard_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0b97fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "lb_data = (\n",
    "    lb_data.merge(\n",
    "        feature_weights.rename(columns=lambda x: \"pre_\" + x), \n",
    "        how=\"left\", \n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    "    .merge(\n",
    "        feature_weights.rename(columns=lambda x: \"post_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    "    .merge(\n",
    "        morph_embeddings.rename(columns=lambda x: \"pre_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    "    .merge(\n",
    "        morph_embeddings.rename(columns=lambda x: \"post_\" + x),\n",
    "        how=\"left\",\n",
    "        validate=\"m:1\",\n",
    "        copy=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074a460",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "lb_data.shape\n",
    "lb_data.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6d054a",
   "metadata": {},
   "source": [
    "### Example Prediction: Simple Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2467bc",
   "metadata": {},
   "source": [
    "Now we fit a our logistic regression model on the full training data and use it to make predictions on the leaderboard data. But first, we need to add in our new engineered feature on neural tuning similarity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089e47f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# compute the cosine similarity between the pre- and post- feature weights \n",
    "lb_data[\"fw_similarity\"] = lb_data.apply(row_feature_similarity, axis=1)\n",
    "# compute the cosine similarity between the pre- and post- morphological embeddings \n",
    "lb_data[\"morphEm_similarity\"] = lb_data.apply(row_morphEmbed_similarity, axis=1)\n",
    "# compute the cosine similarity between the X-coordinates \n",
    "lb_data[\"coorX_difference\"] = lb_data.apply(xCoorDifference, axis=1)\n",
    "# compute the cosine similarity between the Y-coordinates \n",
    "lb_data[\"coorY_difference\"] = lb_data.apply(yCoorDifference, axis=1)\n",
    "# compute the cosine similarity between the Z-coordinates \n",
    "lb_data[\"coorZ_difference\"] = lb_data.apply(zCoorDifference, axis=1)\n",
    "lb_data[\"theta_btwn_2neurons\"] = lb_data.apply(thetaDiff, axis = 1)\n",
    "# 3D distance between axon and dendrite \n",
    "lb_data[\"3Dax_denDist\"] = lb_data.apply(EDax_denDist, axis=1)\n",
    "# 3D distance between pre and post nucleus distance\n",
    "lb_data[\"3Dpre_postNucleusDist\"] = lb_data.apply(EDpre_postnucleusDist, axis=1)\n",
    "# Pre radial location \n",
    "lb_data[\"preRadialLocation\"] = lb_data.apply(preRadialLocation, axis=1)\n",
    "# Post radial location \n",
    "lb_data[\"postRadialLocation\"] = lb_data.apply(postRadialLocation, axis=1)\n",
    "# pre and post oracle difference \n",
    "lb_data[\"Combined_oracle\"] = lb_data.apply(oracleDiff, axis=1)\n",
    "# pre and post test score difference \n",
    "lb_data[\"Combined_testScore\"] = lb_data.apply(testScoreDiff, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64f2cf67",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/DomDuliepre/Desktop/ELEC-578/ELEC-578_COMP./neuron-synapse-prediction/SUBs/ELEC-578_CompNotebook_SUB_0.75751.ipynb Cell 55\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/DomDuliepre/Desktop/ELEC-578/ELEC-578_COMP./neuron-synapse-prediction/SUBs/ELEC-578_CompNotebook_SUB_0.75751.ipynb#Y105sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m predict_features \u001b[39m=\u001b[39m numerical_features \u001b[39m+\u001b[39m categorical_features\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/DomDuliepre/Desktop/ELEC-578/ELEC-578_COMP./neuron-synapse-prediction/SUBs/ELEC-578_CompNotebook_SUB_0.75751.ipynb#Y105sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Split the data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/DomDuliepre/Desktop/ELEC-578/ELEC-578_COMP./neuron-synapse-prediction/SUBs/ELEC-578_CompNotebook_SUB_0.75751.ipynb#Y105sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m X \u001b[39m=\u001b[39m data[predict_features]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/DomDuliepre/Desktop/ELEC-578/ELEC-578_COMP./neuron-synapse-prediction/SUBs/ELEC-578_CompNotebook_SUB_0.75751.ipynb#Y105sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m y \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mconnected\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/DomDuliepre/Desktop/ELEC-578/ELEC-578_COMP./neuron-synapse-prediction/SUBs/ELEC-578_CompNotebook_SUB_0.75751.ipynb#Y105sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, RepeatedStratifiedKFold, StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.impute import SimpleImputer\n",
    "import xgboost as xgb\n",
    "\n",
    "# Splitting features into numerical, categorical, and one to impute\n",
    "numerical_features = [\"fw_similarity\", \"adp_dist\", \"dendritic_coor_x\", \"dendritic_coor_y\", \n",
    "                    \"post_nucleus_id\", \"pre_nucleus_id\", \"dendritic_coor_z\", \"axonal_coor_x\", \n",
    "                    \"axonal_coor_y\", \"axonal_coor_z\", \"post_skeletal_distance_to_soma\", \n",
    "                    \"pre_skeletal_distance_to_soma\", \"pre_test_score\", \"pre_rf_x\", \n",
    "                    \"pre_rf_y\", \"post_test_score\", \"post_rf_x\", \"post_rf_y\", \"theta_btwn_2neurons\",\n",
    "                    \"coorX_difference\", \"coorY_difference\", \"Combined_oracle\", \"Combined_testScore\",\n",
    "                    \"coorZ_difference\", \"morphEm_similarity\", \"pre_nucleus_x\", \"post_nucleus_x\",\n",
    "                    \"pre_nucleus_y\", \"post_nucleus_y\", \"pre_nucleus_z\", \"post_nucleus_z\"\n",
    "]\n",
    "categorical_features = [\"compartment\", \"pre_brain_area\", \"post_brain_area\"]\n",
    "\n",
    "# Preprocessing: ColumnTransformer will handle encoding, scaling, and imputing for machine learning models\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features),\n",
    "        ('impute', SimpleImputer(strategy='median'), ['morphEm_similarity'])\n",
    "    ])\n",
    "\n",
    "predict_features = numerical_features + categorical_features\n",
    "\n",
    "# Split the data\n",
    "X = data[predict_features]\n",
    "y = data[\"connected\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=5)\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Preprocessing: ColumnTransformer will handle encoding, scaling, and imputing for machine learning models\n",
    "categorical_indices = [X_train.columns.get_loc(col) for col in categorical_columns]\n",
    "#morphEm_similarity_index = X_train.columns.get_loc(\"morphEm_similarity\")\n",
    "\n",
    "# for handling imbalanced data\n",
    "resampler = RandomOverSampler(random_state=5)\n",
    "\n",
    "# Model definitions with a reduced parameter grid for efficiency\n",
    "models_and_parameters = {\n",
    "    'XGBoost': {\n",
    "        'model': xgb.XGBClassifier(), \n",
    "        'params': {\n",
    "            'model__n_estimators': [300],\n",
    "            'model__learning_rate': [0.0001],\n",
    "            'model__max_depth': [7],  # Changed values to integers\n",
    "            'model__subsample': [1.0], \n",
    "            'model__colsample_bytree': [0.3],\n",
    "            'model__gamma': [0.09]\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "for model_name, mp in models_and_parameters.items():\n",
    "    # Create a pipeline that first applies the preprocessor, then resamples, and then runs the classifier\n",
    "    pipeline = Pipeline([('preprocessor', preprocessor),\n",
    "                         ('resampler', resampler),\n",
    "                         ('model', mp['model'])])\n",
    "\n",
    "    stratified_split = StratifiedShuffleSplit(n_splits=5, test_size=0.3, random_state=5) # Changed from RepeatedStratifiedKFold\n",
    "    clf = GridSearchCV(pipeline, mp['params'], cv=stratified_split, scoring='balanced_accuracy', n_jobs=-1)#, n_iter=10)  # n_iter\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Output results\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"Best hyperparameters: {clf.best_params_}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy:.4f}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Convert the cv_results_ dictionary to a DataFrame for easier manipulation\n",
    "cv_results_df = pd.DataFrame(clf.cv_results_)\n",
    "\n",
    "# If you have a separate leaderboard dataset, you can use the following:\n",
    "lb_data[\"pred\"] = clf.best_estimator_.predict_proba(lb_data[predict_features])[:, 1]\n",
    "lb_data[\"connected\"] = lb_data[\"pred\"] > .5\n",
    "\n",
    "#XGBoost:\n",
    "#Best hyperparameters: {'model__colsample_bytree': 0.2, 'model__gamma': 0.001, 'model__learning_rate': 0.0001, 'model__max_depth': 6, 'model__n_estimators': 135, 'model__subsample': 1.0}\n",
    "#Accuracy: 0.7599\n",
    "#Balanced Accuracy: 0.7575\n",
    "# Sub 0.75304\n",
    "\n",
    "#XGBoost:\n",
    "#Best hyperparameters: {'model__colsample_bytree': 0.2, 'model__gamma': 0.001, 'model__learning_rate': 0.0001, 'model__max_depth': 6, 'model__n_estimators': 135, 'model__subsample': 1.0}\n",
    "#Accuracy: 0.7599\n",
    "#Balanced Accuracy: 0.7600\n",
    "\n",
    "#XGBoost:\n",
    "#Best hyperparameters: {'model__colsample_bytree': 0.2, 'model__gamma': 0.001, 'model__learning_rate': 0.0001, 'model__max_depth': 6, 'model__n_estimators': 130, 'model__subsample': 1.0}\n",
    "#Accuracy: 0.7649\n",
    "#Balanced Accuracy: 0.7550\n",
    "# Sub 0.75579\n",
    "\n",
    "#XGBoost:\n",
    "#Best hyperparameters: {'model__colsample_bytree': 0.3, 'model__gamma': 0.09, 'model__learning_rate': 0.0001, 'model__max_depth': 7, 'model__n_estimators': 300, 'model__subsample': 1.0}\n",
    "#Accuracy: 0.7708\n",
    "#Balanced Accuracy: 0.7592\n",
    "# Sub 0.75751"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert the cv_results_ dictionary to a DataFrame for easier manipulation\n",
    "cv_results_df = pd.DataFrame(clf.cv_results_)\n",
    "\n",
    "# Function to plot accuracy vs a single hyperparameter (or list)\n",
    "def plot_hyperparameter_vs_metric(dataframe, hyperparam, metric):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.pointplot(x=hyperparam, y=metric, data=dataframe)\n",
    "    plt.title(f'{metric} vs {hyperparam}')\n",
    "    plt.xlabel(hyperparam)\n",
    "    plt.ylabel(metric)\n",
    "    plt.show()\n",
    "\n",
    "# Example: Plotting accuracy vs max_depth\n",
    "plot_hyperparameter_vs_metric(cv_results_df, \"param_model__max_depth\", \"mean_test_score\")\n",
    "# Example: Plotting accuracy vs learning_rate\n",
    "plot_hyperparameter_vs_metric(cv_results_df, \"param_model__learning_rate\", \"mean_test_score\")\n",
    "plot_hyperparameter_vs_metric(cv_results_df, \"param_model__subsample\", \"mean_test_score\")\n",
    "plot_hyperparameter_vs_metric(cv_results_df, \"param_model__colsample_bytree\", \"mean_test_score\")\n",
    "#plot_hyperparameter_vs_metric(cv_results_df, \"param_model__gamma\", \"mean_test_score\")\n",
    "plot_hyperparameter_vs_metric(cv_results_df, \"param_model__n_estimators\", \"mean_test_score\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c96a3f",
   "metadata": {},
   "source": [
    "### Example of Creating a Leaderboard Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771bb7d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#columns should be ID, connected\n",
    "submission_data = lb_data.filter(['ID','connected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad284b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter notebook failed to launch. \n",
      "\u001b[1;31mKernel not initialized in Session. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#writing csv files\n",
    "submission_data.to_csv('example_submission_data.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
